# The Society for AI Safety

*A Proposal for Professional Standards in the Age of Powerful AI*

**Draft, January 2026**

---

#### The Problem

AI safety, as a field, exists because we recognize that powerful AI systems pose risks to humanity. We study alignment, robustness, interpretability, and governance because we believe AI should serve human flourishing, not undermine it.

This work depends on conditions we often take for granted: freedom of inquiry, rule of law, the dignity of the human person. Without these, there is no independent AI safety research. A field devoted to ensuring AI respects human interests cannot do its work in a society that does not.

But our field lacks something that other professions take for granted: a shared ethical framework with institutional backing. Doctors have the Hippocratic oath and medical boards. Engineers have professional societies with codes of conduct. Lawyers have bar associations that can sanction members who violate professional standards.

AI practitioners have none of this. When an individual researcher is asked to build systems that could be used to coerce, surveil, or manipulate people at scale, they face that pressure alone. They can refuse, but they bear all the risk. There is no collective voice to appeal to, no professional standard to cite, no institution that will back them up.

This is a problem in ordinary times. When powerful actors seek to use AI to coerce populations, it becomes urgent.

---

#### The Core Insight

AI safety asks whether systems are safe. But safe for whom?

Traditional alignment research asks: does the AI do what its operator intends? This is necessary but not sufficient. An AI system perfectly aligned with one human's intentions, used to surveil, manipulate, or coerce other humans, is not safe for those others. Alignment with an operator is not the same as safety for humanity.

True AI safety requires asking not just "does this system do what its operator intends?" but "does this system respect the dignity and agency of the humans it affects?"

Leading AI researchers already recognize this. Dario Amodei has identified ["misuse for power-seizing"](https://www.darioamodei.com/essay/the-adolescence-of-technology) as one of the fundamental risks of powerful AI: autocratic actors using AI for surveillance, propaganda, and coercion at scale. This is not a speculative concern. It is a first-class safety risk, as central to the field as alignment or robustness. AI-enabled mass surveillance and propaganda are, in Amodei's words, "bright red lines" that must not be crossed even within democracies.

When AI enables some humans to coerce others at scale, it has failed the test of safety, however well it serves its operator. A system that faithfully executes the goal of suppressing dissent or manipulating elections is not safe. It is the opposite of safe. The humans being coerced have dignity and interests too.

This does not mean opposing the development of powerful AI. Powerful AI, developed responsibly, can strengthen democratic institutions and expand human capability. The point is that power requires constraint. The more capable our systems become, the more important it is that they serve all of the people they affect, not just the people who control them.

This means AI safety practitioners have ethical obligations that extend beyond technical competence. We have an obligation not to build tools that coerce populations, even if asked. We have an obligation to consider whose dignity and agency our systems serve and whose they diminish. We have an obligation to transparency: to disclose concerning behaviors and system limitations rather than conceal them. And we have an obligation to each other: to create conditions where refusing such work is possible.

These obligations are implicit in the values that draw people to AI safety. The Society for AI Safety proposes to make them explicit, shared, and institutionally supported.

---

#### Principles

Members of the Society for AI Safety affirm the following principles:

**1. Safety means safety for everyone affected.** An AI system is not safe if it serves one human's intentions by coercing others. We assess safety not just from the operator's perspective, but from the perspective of all humans the system affects.

**2. Human dignity and agency are foundational.** AI systems should expand human capability and choice, not diminish them. Systems designed to manipulate, deceive, or coerce humans at scale violate the dignity of the people they target, regardless of how well they serve their operators.

**3. Power requires constraint.** We are not opposed to building powerful AI. But the more capable systems become, the more essential it is that they are subject to oversight, transparency, and accountability. Power without constraint is the definition of a safety failure.

**4. Liberal democratic conditions are preconditions for our work.** Freedom of speech, rule of law, and respect for the human person are not incidental to AI safety. They are the conditions under which independent safety research is possible. We have a professional interest, not just a moral one, in defending these principles.

**5. Professional judgment requires independence.** AI safety practitioners must be free to assess risks honestly and refuse work that violates professional ethics, without fear of retaliation.

**6. Collective responsibility enables individual integrity.** No individual can resist institutional pressure alone. We commit to supporting each other when professional ethics conflict with employer demands.

---

#### Standards of Conduct

Members commit to the following standards:

**We will not build** AI systems whose primary purpose is to coerce, manipulate, or surveil populations, including systems designed to:
- Monitor populations to suppress dissent or punish disfavored groups
- Manipulate people's beliefs or decisions through deception at scale
- Target individuals based on political beliefs, identity, or associations
- Make lethal decisions without meaningful human accountability
- Deceive users about whether they are interacting with AI

**We will ask** who is affected by the systems we build, and whether those people's dignity and agency are respected or diminished.

**We will assess** the foreseeable uses of systems we work on, not just their intended uses, and decline to participate in work where coercion is the likely outcome.

**We will be transparent** about the capabilities and limitations of systems we build, and speak honestly about risks we perceive, both internally and, when appropriate, publicly, even when this is uncomfortable for employers or funders.

**We will support colleagues** who face retaliation for adhering to these standards.

---

#### Membership

**Fellows** are established researchers and practitioners recognized for significant contributions to AI safety. Fellows are elected by existing Fellows and pay no dues. They form the initial core of the Society and provide credibility and continuity.

**Members** are AI practitioners who affirm the Society's principles and standards. Membership is open to anyone working in AI research, development, policy, or governance. Annual dues support the Society's operations and mutual protection fund.

**Student and transitioning members** pay reduced dues and have full membership rights.

Membership is a commitment, not just an affiliation. Members agree to uphold the standards of conduct and may be removed for serious violations after review.

---

#### Benefits

**Ethical credential.** Membership signals commitment to professional standards. Members may use post-nominal letters (MSAIS) and are listed in a public directory. Journalists, policymakers, and employers can verify membership.

**Mutual protection fund.** The Society maintains a fund to support members who face professional retaliation for adhering to ethical standards. This may include legal defense, temporary income support, and assistance finding new positions. This is the core institutional benefit: it changes the calculus for individuals facing pressure to do unethical work.

**Collective voice.** The Society issues statements on matters of professional concern, represents members in policy discussions, and provides institutional backing for members who cite professional standards in refusing work.

**Ethics consultation.** Members may request confidential guidance from an ethics committee on difficult professional situations.

**Job board.** Employers seeking AI practitioners committed to ethical standards can post positions. The Society may vet employers for basic adherence to professional norms.

**Annual conference.** A gathering focused on AI safety practice, including both technical and ethical dimensions. Builds community and provides professional development.

**Mentorship and community.** Senior practitioners mentor junior ones. Local chapters in major hubs. Private forums for member discussion.

---

#### Governance

The Society is governed by an elected Council, with representation from Fellows, Members, and Student members. The Council sets policy, oversees the mutual protection fund, and elects new Fellows.

An Ethics Committee, appointed by the Council, handles confidential consultations and reviews complaints about member conduct.

The Society is funded by dues and donations. It accepts no funding from sources that would compromise its independence.

---

#### Why Now

The AI systems being built today are powerful enough to reshape society. They can be tools of liberation or tools of control. The choice depends in part on the people who build them.

Many AI safety researchers are not political people. They are scientists and engineers whose deep loyalties are to the principles of classical liberal democracy: freedom of speech, rule of law, the dignity of the human person. Many came to this work, or came to this country, because they believed in those principles. Their bar for political engagement is high.

But the same principles that draw people to AI safety require defending the conditions under which that work is possible. When the rule of law is weakened, when oversight is dismantled, when powerful systems are deployed to coerce rather than serve, the foundations of our field are at stake. There comes a point where silence is not a neutral professional stance.

This proposal is not for partisan advocates. It is for non-partisan experts whose professional standards and foundational values compel them to speak. It provides a community, a shared framework, and institutional support so that acting on professional conscience does not require acting alone.

The principles articulated here are not partisan. The belief that AI should not be used to coerce people is shared across the political spectrum. The specific threats of any given moment may come from a particular direction, but the standards we establish apply regardless of who holds power. AI that coerces populations is unsafe whether it serves the left or the right.

If you work in AI and believe that your work should serve human flourishing, not undermine it, we invite you to join.

---

#### Next Steps

This proposal is a starting point. To become real, it needs:

**Founding Fellows.** A core group of credible researchers willing to lend their names and help shape the Society's direction. If you are a senior AI safety researcher interested in serving as a founding Fellow, please reach out.

**Founding Members.** Practitioners who will join, pay dues, and build the community from the start.

**Organizational support.** Incorporating as a nonprofit, setting up governance, establishing the mutual protection fund. This requires legal and administrative work.

**Feedback.** This draft will improve with input. If you have suggestions, criticisms, or questions, please contribute.

---

#### Contribute

This proposal is part of [Poets, Nurses, and Programmers](index.md). Pull requests welcome on [GitHub](https://github.com/davidbau/poetsandnurses).

---

*"The degree of liberty or tyranny in any government is in large degree a reflection of the relative determination of the subjects to be free and their willingness and ability to resist efforts to enslave them." â€” Gene Sharp*
